{"posts":[{"title":"Hexo常用指令大全","text":"Hexo 常用命令速查表 命令 功能 常用参数/示例 初始化 npm install -g hexo-cli 全局安装Hexo命令行工具 hexo init &lt;folder&gt; 初始化博客项目 hexo init myblog npm install 安装依赖包（在项目目录执行） 内容管理 hexo new &quot;标题&quot; 新建文章 hexo new &quot;Hello World&quot; hexo new page &quot;名称&quot; 新建页面 hexo new page &quot;about&quot; hexo publish &lt;filename&gt; 发布草稿 hexo publish draft/untitled.md 生成与预览 hexo generate 生成静态文件（简写hexo g） hexo g --watch（监听文件变化） hexo server 启动本地服务器（简写hexo s） hexo s -p 5000（指定端口） hexo clean 清除缓存和生成文件 常与生成命令组合使用 部署 npm install hexo-deployer-git --save 安装Git部署插件 hexo deploy 部署到服务器（简写hexo d） hexo d --generate（先生成后部署） 组合命令 hexo g -d 生成后立即部署 常用部署组合 hexo s -g 生成后启动服务器 开发调试常用 高级操作 hexo list &lt;type&gt; 列出所有文章/页面等 hexo list post hexo version 查看Hexo版本 hexo --config custom.yml 使用自定义配置文件 多环境配置时使用 典型工作流示例 12345678# 1. 创建新文章hexo new &quot;深入理解Hexo架构&quot;# 2. 本地写作并预览hexo clean &amp;&amp; hexo g &amp;&amp; hexo s# 3. 部署到GitHubhexo clean &amp;&amp; hexo g -d 配置注意要点 部署配置（_config.yml） 1234deploy: type: git repo: https://github.com/用户名/仓库名.git branch: gh-pages 主题配置示例 1theme: icarus # 需先安装主题到themes目录 常用插件推荐 插件 功能 安装命令 hexo-abbrlink 生成永久链接 npm install hexo-abbrlink --save hexo-all-minifier 压缩静态资源 npm install hexo-all-minifier --save hexo-generator-search 添加本地搜索 npm install hexo-generator-search --save 掌握这些命令可提升博客管理效率，建议结合--debug参数排查问题： 1hexo g --debug # 显示详细生成日志 icarus教程:https://ppoffice.github.io/hexo-theme-icarus/categories/Widgets/","link":"/posts/Hexo%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E5%A4%A7%E5%85%A8/"},{"title":"Mysql核心框架","text":"Mysql核心框架 本文旨在梳理和理解 MySQL 的一些核心知识点，并结合常见面试题进行思考和总结。这些内容主要来源于我的个人学习与理解。 1. 事务 概念 事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 那抹什么是ACID呢？ ACID 原子性（Atomicity） ​ 事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。 ​ 回滚可以用回滚日志（Undo Log）来实现，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistency） ​ 数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对同一个数据的读取结果都是相同的。 ​ 我个人理解就是这个执行结果要合理，不能我转了你500，你拿到了1000这种状态。 隔离性（Isolation） ​ 一个事务所做的修改在最终提交以前，对其它事务是不可见的。 持久性（Durability） ​ 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 ​ 系统发生崩溃可以用重做日志（·Redo Log·）进行恢复，从而实现持久性。与回滚日志记录数据的逻辑修改不同，重做日志记录的是数据页的物理修改。 举个例子: 例如，A向B转账500元，这个操作要么都成功，要么都失败，体现了原子性。转账过程中数据要保持一致，A扣除了500元，B必须增加500元。隔离性体现在A向B转账时，不受其他事务干扰。持久性体现在事务提交后，数据要被持久化存储。 事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对系统崩溃的情况。 原子性和隔离性直接保证一致性，简介保证执行结果的正确。而一致性是觉得执行结果正确的直接因素。 持久性用于应对系统的崩溃，提高数据库的高可用性和鲁棒性。 2. 并发一致性问题 注意w为write操作,r为read操作 丢失修改 T1 T2 w a=10 w a=20 r a=20 r a=20 可以看到我们的T1线程和T2线程并发执行的时候T1丢失了原来a=10的操作痕迹，T2后提交覆盖了T1的提交，似乎T1没有执行w a=10的操作一样，这就是&quot;丢失修改&quot;。 读脏数据 T1 T2 r a=10 w a=20 r a=20 rollback T1,T2并发执行在前面都没问题，可是当T1rollback操作后数据库里面的值又变回了10，此时T2拿到并不是数据库真实的数据，如果拿着这个数据执行支付操作可想而知会给我们的系统造成多大的损失。 不可重复读 T1 T2 r a=10 w a=20 r a=20 其实不可重复读很好理解，就是在同一个事务中我进行两次读取操作结果获得值不相同，也可能会对我们的系统造成很大的困扰。 幻影读(幻读) T1 T2 count(user_id) 1 delete(user_object) count(user_id) 0 幻读的出现场景主要在聚合函数统计时候出现的概率比较大一些。一个事务下的两次聚合操作竟然不相同，如果是在秒杀场景里面，可能就造成了超买超卖问题。 3. 封锁 锁的粒度 MySQL在粒度上分类的话分为行级锁、表级锁、页面锁 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 从性能上看的话，锁的粒度越细，能够并发的线程数量就越多，性能就越高。反之则性能越低。所以从这个层面上考虑的行级锁的性能是高于标记锁的。 三种常用引擎的支持情况 InnoDB MyISAM MEMORY 行级锁和表级锁 支持表级锁 支持标记锁 从这也可以看出来InnoDB在锁的性能上是高于其他两个存储引擎的。 表级锁 基本的两大类表锁 ＭySQL的表锁有两种模式：表共享读锁（Table Read Lock）和表独占写锁（Table Write Lock） 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作； 当然有些存储引擎还支持意向锁，如我们大名鼎鼎的InnoDB。 是否阻塞 读锁 写锁 读锁 Y N 写锁 N N tip：读读兼容，读写阻塞，写写阻塞 行级锁 基本的两大类行锁 共享锁（S 锁，Shared Lock） 允许多个事务同时读一行，但不能写。 排他锁（X 锁，Exclusive Lock） 一个事务获取排他锁后，别人既不能读也不能写（读是指加锁的读）。 常见在 UPDATE、DELETE、INSERT 操作里会自动加。 InnoDB 特有的“变种”行锁 为了处理复杂的并发场景，InnoDB 在基本 S/X 锁上又扩展出几种： 记录锁（Record Lock） 最常见的行锁，就是锁住某一条索引记录。 前提是查询必须用到索引，否则可能升级成表锁。 间隙锁（Gap Lock） 锁住某个范围的“间隙”，而不是已有的行。 用来防止“幻读”（别人插入新行导致范围查询结果变了）。 例如： 1SELECT * FROM user WHERE age BETWEEN 20 AND 30 FOR UPDATE; FOR UPDATE → 给匹配行加排他锁。 BETWEEN 20 AND 30 → 锁住这个范围的记录。 临键锁（Next-Key Lock） 记录锁 + 间隙锁的组合，锁住“某一条记录及其前面的间隙”。 这是 InnoDB 在 可重复读（RR） 隔离级别下的默认模式，主要就是为了防止幻读。 InnoDB默认情况下使用的就是行锁，如果要表锁的话必须显式加锁。 InnoDB与MyISAM的最大不同有两点： 一是支持事务（TRANSACTION）； 二是采用了行级锁。 行锁定是对索引记录的锁定。 1SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; 防止任何其他事务插入，更新或删除t.c1值为10的行。 MySQL 的 UPDATE 在并发情况下，如果是 InnoDB 引擎，会通过行级排他锁避免同一行被多个事务同时修改。这也是Mysql层面上的解决超买超卖的一种手段捏。 行锁升级为表锁 行锁也有可能升级成表锁，具体的情况： 不适用索引的情况下加锁 使用普通索引的情况下加锁（tips:普通索引不限制值单纯建立b+树来加快查询速度，随便插,咱普遍说的索引一般是主键索引和唯一索引有非重复的需求) 范围性查询 总结行锁是建立在索引的基础上的，普通索引的数据重复率过高或导致索引失效，行锁升级为表锁 封锁的类型 读写锁 互斥锁（Exclusive），简写为 X 锁，又称写锁。 共享锁（Shared），简写为 S 锁，又称读锁。 有以下两个规定： 一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。 一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。 是否阻塞 读锁 写锁 读锁 Y N 写锁 N N 也就是上面行级锁里面的规定 意向锁 使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。 在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。 意向锁在原来的 X/S 锁之上引入了 IX/IS，IX/IS 都是表锁，用来表示一个事务想要在表中的某个数据行上加 X 锁或 S 锁。有以下两个规定： 一个事务在获得某个数据行对象的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行对象的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 4. 封锁协议 一共有三层封锁协议 一级封锁协议 事务 T 要修改数据 A 时必须加 X 锁，直到 T 结束才释放锁。 T1 T2 lock-x a w a=10 lock-x a commit unlock-x a get lock-x w a=20 commit unlock-x a 可以看到，因为每次只有一个事务可以进行写操作，所以我们的事务修改就不会被覆盖 二级封锁协议 在一级的基础上，要求读取数据 A 时必须加 S 锁，读取完马上释放 S 锁。 T1 T2 lock-x a r a=10 w a=20 lock-s a rollback unlock-x a get lock-s r a=10 unlock-s a 可以解决读脏数据问题，因为如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据,用的读写阻塞来解决 三级封锁协议 在二级的基础上，要求读取数据 A 时必须加 S 锁，直到事务结束了才能释放 S 锁。 T1 T2 lock-x a r a=10 r a=10 lock-x a rollback unlock-x a get lock-x w a=20 commit unlock-x a 可以解决不可重复读的问题，因为读 A 时，其它事务不能对 A 加 X 锁，从而避免了在读的期间数据发生改变。 两段锁协议 加锁和解锁分为两个阶段进行。 可串行化调度是指，通过并发控制，使得并发执行的事务结果与某个串行执行的事务结果相同。串行执行的事务互不干扰，不会出现并发一致性问题。 事务遵循两段锁协议是保证可串行化调度的充分条件。例如以下操作满足两段锁协议，它是可串行化调度。 1lock-x(A)...lock-s(B)...lock-s(C)...unlock(A)...unlock(C)...unlock(B) 但不是必要条件，例如以下操作不满足两段锁协议，但它还是可串行化调度。 1lock-x(A)...unlock(A)...lock-s(B)...unlock(B)...lock-s(C)...unlock(C) MySQL 的 InnoDB 存储引擎采用两段锁协议，会根据隔离级别在需要的时候自动加锁，并且所有的锁都是在同一时刻被释放，这被称为隐式锁定。 InnoDB 也可以使用特定的语句进行显示锁定： 12SELECT ... LOCK In SHARE MODE; #加读锁SELECT ... FOR UPDATE;#加排他锁 5.隔离级别 未提交读（READ UNCOMMITTED） 事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED） 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。 因为在一个事务提交前的数据是对其他事务不可见的，所以在数据没有确定存入数据库前是不会被其他事务看见的，因此解决了数据被读到是修改后的值，但是另一个事务rollback修改，导致事务中读取到的数据与数据库中的数据不一致问题。即为解决了脏读问题。 可重复读（REPEATABLE READ） 保证在同一个事务中多次读取同一数据的结果是一样的。 可串行化（SERIALIZABLE） 强制事务串行执行，这样多个事务互不干扰，不会出现并发一致性问题。 该隔离级别需要加锁实现，因为要使用加锁机制保证同一时间只有一个事务执行，也就是保证事务串行执行。 隔离级别能解决的并发一致性问题 脏读 不可重复读 幻影读 未提交读 × × × 提交读 √ × × 可重复度 √ √ × 可串行化 √ √ √ 6. MVCC 多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。 基本思想 在封锁一节中提到，加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的，而 MVCC 利用了多版本的思想，写操作更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite读写分离 类似。 在 MVCC 中事务的修改操作（DELETE、INSERT、UPDATE）会为数据行新增一个版本快照。 脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。因为没有使用 START TRANSACTION 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。 Undo 日志 MVCC 的多版本指的是多个版本的快照，快照存储在 Undo 日志中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。 例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。 123INSERT INTO t(id, x) VALUES(1, &quot;a&quot;);UPDATE t SET x=&quot;b&quot; WHERE id=1;UPDATE t SET x=&quot;c&quot; WHERE id=1; 因为没有使用 START TRANSACTION 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。 INSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1。 ReadView MVCC 维护了一个 ReadView 结构，主要包含了当前系统未提交的事务列表 TRX_IDs {TRX_ID_1, TRX_ID_2, …}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。 在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用： TRX_ID &lt; TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。 TRX_ID &gt; TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。 TRX_ID_MIN &lt;= TRX_ID &lt;= TRX_ID_MAX，需要根据隔离级别再进行判断： 提交读：该隔离界别下每一次查询都要进行一次快照读(readView)操作，如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。 可重复读：**该隔离界别下事务开始的时候进行一次快照读操作(readView)而且只进行这一次，**如果 TRX_ID 在 TRX_IDs 列表中就不能使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。 在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。 7. 范式 函数依赖 记 A-&gt;B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。 如果 {A1，A2，… ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。 对于 A-&gt;B，如果能找到 A 的真子集 A’，使得 A’-&gt; B，那么 A-&gt;B 就是部分函数依赖，否则就是完全函数依赖。 对于 A-&gt;B，B-&gt;C，则 A-&gt;C 是一个传递函数依赖。 异常 以下的学生课程关系的函数依赖为 {Sno, Cname} -&gt; {Sname, Sdept, Mname, Grade}，键码为 {Sno, Cname}。也就是说，确定学生和课程之后，就能确定其它信息。 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 不符合范式的关系，会产生很多异常，主要有以下四种异常： 冗余数据：例如 学生-2 出现了两次。 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。 删除异常：删除一个信息，那么也会丢失其它信息。例如删除了 课程-1 需要删除第一行和第三行，那么 学生-1 的信息就会丢失。 插入异常：例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。 范式 范式理论是为了解决以上提到四种异常。 高级别范式的依赖于低级别的范式，1NF 是最低级别的范式。 1. 第一范式 (1NF) 属性不可分。 2. 第二范式 (2NF) 每个非主属性完全函数依赖于键码。 可以通过分解来满足。 分解前 Sno Sname Sdept Mname Cname Grade 1 学生-1 学院-1 院长-1 课程-1 90 2 学生-2 学院-2 院长-2 课程-2 80 2 学生-2 学院-2 院长-2 课程-1 100 3 学生-3 学院-2 院长-2 课程-2 95 以上学生课程关系中，{Sno, Cname} 为键码，有如下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname Sno, Cname-&gt; Grade Grade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。 Sname, Sdept 和 Mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。 分解后 关系-1 Sno Sname Sdept Mname 1 学生-1 学院-1 院长-1 2 学生-2 学院-2 院长-2 3 学生-3 学院-2 院长-2 有以下函数依赖： Sno -&gt; Sname, Sdept Sdept -&gt; Mname 关系-2 Sno Cname Grade 1 课程-1 90 2 课程-2 80 2 课程-1 100 3 课程-2 95 有以下函数依赖： Sno, Cname -&gt; Grade 3. 第三范式 (3NF) 非主属性不传递函数依赖于键码。 上面的 关系-1 中存在以下传递函数依赖： Sno -&gt; Sdept -&gt; Mname 可以进行以下分解： 关系-11 Sno Sname Sdept 1 学生-1 学院-1 2 学生-2 学院-2 3 学生-3 学院-2 关系-12 Sdept Mname 学院-1 院长-1 学院-2 院长-2","link":"/posts/Mysql%E6%A0%B8%E5%BF%83%E6%A1%86%E6%9E%B6%E7%9F%A5%E8%AF%86/"},{"title":"redis基础结构","text":"Redis入门 （NoSQL, Not Only SQL） 非关系型数据库 关系型数据库：以 表格 的形式存在，以 行和列 的形式存取数据，一系列的行和列被称为表，无数张表组成了 数据库。支持复杂的 SQL 查询，能够体现出数据之间、表之间的关联关系；也支持事务，便于提交或者回滚。 非关系型数据库：以 key-value 的形式存在，可以想象成电话本的形式，人名（key）对应电话号码（value）。不需要写一些复杂的 SQL 语句，不需要经过 SQL 的重重解析，性能很高；可扩展性也比较强，数据之间没有耦合性，需要新加字段就直接增加一个 key-value 键值对即可。 Redis 是 速度极快的、基于内存的，键值型 NoSQL 数据库。 为什么这么快？ 完全基于内存操作。 使用非阻塞的 IO 多路复用机制。 数据结构简单，对数据操作也简单。 使用单线程，避免了上下文切换和竞争产生的消耗。 支持多种数据类型，包括 String、Hash、List、Set、ZSet 等。 IO 多路复用机制 Redis 使用的是 IO 多路复用机制 来处理 高并发请求，这使得它能在 单线程 模式下仍然保持高吞吐量。 🔹 Redis 为什么要用 IO 多路复用？ Redis 是单线程的，但仍然能高效处理大量连接，这依赖于 IO 多路复用。 传统的 阻塞 IO 方式，每次只能处理一个连接，性能受限。 多路复用可以 同时监听多个客户端请求，只处理活跃连接，减少 CPU 空转。 🔹 Redis 的 IO 多路复用机制 Redis 采用 epoll（Linux）或 select（Windows） 作为 IO 多路复用技术，主要使用 aeEventLoop 事件处理机制： 主线程通过 epoll/select/kqueue 监听多个客户端连接 当某个连接有数据可读（如命令请求），Redis 触发相应的回调函数 回调函数读取请求，处理命令，返回结果 继续监听新的请求，不会阻塞在某个请求上 Redis 使用 事件驱动模型，主要有： 可读事件（AE_READABLE）：当客户端有数据可读时触发。 可写事件（AE_WRITABLE）：当客户端可以写数据时触发。 文件事件（File Event）：通过 epoll 监听 多个 socket 连接。 时间事件（Time Event）：用于定时任务（比如 key 过期检测）。 🔹 Redis 多路复用示意图 1234567891011[多个客户端] │ ▼[epoll/select 监听] │ ├── 客户端 A 可读 -&gt; 触发回调 -&gt; 读取数据 ├── 客户端 B 可写 -&gt; 触发回调 -&gt; 发送数据 ├── 客户端 C 可读 -&gt; 触发回调 -&gt; 读取数据 │ ▼[主线程执行 Redis 命令逻辑] Redis的基础结构类型 Key结构 让 Redis 的 key 形成层级结构，使用 : 隔开：项目名:业务名:类型:id。 123set blog:user:1 '{&quot;id&quot;:1, &quot;name&quot;:&quot;Jack&quot;, &quot;age&quot;:22}'set blog:user:2 '{&quot;id&quot;:2, &quot;name&quot;:&quot;Mike&quot;, &quot;age&quot;:23}'set blog:article:1 '{&quot;id&quot;:1, &quot;title&quot;:&quot;Spring&quot;}' String类型 Key Value blog:user:1 ‘{“id”:1, “name”:“Jack”, “age”:22}’ blog:user:2 ‘{“id”:2, “name”:“Mike”, “age”:23}’ 分配策略： Java 的 String 是不可变的，无法修改。Redis 的 String 是动态的，可以修改的。Redis 的 String 在内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配。如图所示，当前字符串实际分配的空间为 capacity，一般高于实际的字符串长度 len。当字符串长度小于 1M 时，扩容是对现有空间的成倍增长；如果长度超过 1M 时，扩容一次只会多增加 1M 的空间。String 的最大长度为 512M。 Hash结构 list结构 List 类似 Java 中的 LinkedList，可以看作一个双向链表（有序可重复）。使用 List 可以对链表的两端进行 push 和 pop 操作、读取单个或多个元素、根据值查找或删除元素、支持正向检索和反向检索。 栈：LPUSH + LPOP 或 RPUSH + RPOP。 队列：LPUSH + RPOP 或 RPUSH + LPOP。 Set结构 SADD key member [member ...] ：向 Set 中添加一个或多个元素。 SMEMBERS key ：获取指定 Set 中的所有元素。 SISMEMBER key member ：判断 Set 中是否存在指定元素。 SCARD key ：返回 Set 中的元素个数。 SREM key member [member ...] ：移除 Set 中的指定元素。 SINTER key [key ...] ：求 n 个 key 间的交集。 SDIFF key [key ...] ：求 n 个 key 间的差集。 SUNION key [key ...] ：求 n 个 key 间的并集。 Redis 的 Set 类似 HashSet，可以看作一个 value 为 null 的 HashMap；其特征也与 HashSet 类似：无序不可重复，支持 交集、并集、差集等功能。 ZSet Redis 的 ZSet 是一个可排序的 Set 集合，类似 ZSet。ZSet 的每一个元素都带有一个 score 属性，可以基于 score 属性对元素排序。 ZADD key [score member ...] ：以 score 为权重向 ZSet 中添加一个或多个元素，如果存在则更新 score。 ZREM key member [member ...] ：删除 ZSet 中的指定元素。 ZCARD key ：返回 ZSet 中的元素个数。 ZSCORE key member ：获取 ZSet 中指定元素的 score 值。 ZADD key [score member ...] ：以 score 为权重向 ZSet 中添加一个或多个元素，如果存在则更新 score。 ZREM key member [member ...] ：删除 ZSet 中的指定元素。 ZCARD key ：返回 ZSet 中的元素个数。 ZSCORE key member ：获取 ZSet 中指定元素的 score 值。 ZRANGEBYSCORE key min max ：按照 score 排序后，获取 指定 score 范围 内的元素。 ZINTER numberKeys key [key ...] ｜ ZDIFF numberKeys key [key ...] ｜ ZUNION numberKeys key [key ...] ：求 n 个 Zset 的交集、差集、并集。 Redis 基础结构及其操作指令总结 基础结构 描述 常用指令 示例 String（字符串） 最基本的数据结构，可以存储字符串、整数或浮点数 SET、GET、INCR、DECR、APPEND、MSET、MGET SET key value，GET key List（列表） 有序集合，允许重复元素，底层为双向链表 LPUSH、RPUSH、LPOP、RPOP、LRANGE LPUSH mylist A B C，LRANGE mylist 0 -1 Set（集合） 无序集合，不允许重复元素 SADD、SREM、SMEMBERS、SISMEMBER SADD myset A B C，SMEMBERS myset Hash（哈希） 类似于对象，存储键值对 HSET、HGET、HGETALL、HDEL HSET user name &quot;Alice&quot;，HGET user name ZSet（有序集合） 具有权重（score）的集合，元素按分数排序 ZADD、ZRANGE、ZREM、ZSCORE ZADD myzset 1 A 2 B，ZRANGE myzset 0 -1 Bitmap（位图） 位级别的存储，用于高效存储和操作二进制数据 SETBIT、GETBIT、BITCOUNT SETBIT mybitmap 10 1，GETBIT mybitmap 10 HyperLogLog 近似去重计数结构，适用于大数据计数 PFADD、PFCOUNT PFADD myhll A B C，PFCOUNT myhll Geo（地理位置） 存储经纬度并计算地理距离 GEOADD、GEODIST、GEORADIUS GEOADD mygeo 120.0 30.0 &quot;place1&quot;，GEODIST mygeo place1 place2 Stream（流） 可持久化的消息队列结构 XADD、XLEN、XREAD XADD mystream * name &quot;Alice&quot;，XREAD COUNT 1 STREAMS mystream 0 这些结构和指令在不同的应用场景中有不同的优势，比如 String 适用于缓存数据，List 适用于消息队列，Set 适用于去重，ZSet 适用于排行榜，Hash 适用于存储对象，Bitmap 适用于用户签到或活跃记录，HyperLogLog 适用于大规模数据去重统计，Geo 适用于地理位置存储，Stream 适用于事件流和消息队列。 java客户端连接redis 使用Jedis 1.导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt;&lt;/dependency&gt; 2.建立连接 12345678910111213141516171819202122232425262728293031public class JedisTest { private Jedis jedis; @BeforeEach void setUp(){ //1.建立连接 jedis = new Jedis(&quot;192.168.200.130&quot;,6379); //2.设置密码 jedis.auth(&quot;1234&quot;); //3.选择库 jedis.select(0); } @Test void testString(){ String result = jedis.set(&quot;name&quot;, &quot;小明&quot;); System.out.println(&quot;result= &quot; + result); String name = jedis.get(&quot;name&quot;); System.out.println(&quot;name= &quot;+name); } @AfterEach void tearDown(){ if(jedis!=null){ jedis.close(); } }} 3.jedis连接池 12345678910111213141516171819public class JedisConnectFactory { private static final JedisPool jedisPool; static{ //配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWait(Duration.ofMillis(1000)); jedisPool = new JedisPool(poolConfig,&quot;192.168.200.130&quot;,6379,1000,&quot;1234&quot;); } public static Jedis getJedis(){ return jedisPool.getResource(); }} 1） JedisConnectionFacotry：工厂设计模式是实际开发中非常常用的一种设计模式，我们可以使用工厂，去降低代的耦合，比如Spring中的Bean的创建，就用到了工厂设计模式 2）静态代码块：随着类的加载而加载，确保只能执行一次，我们在加载当前工厂类的时候，就可以执行static的操作完成对 连接池的初始化 3）最后提供返回连接池中连接的方法. 使用springDataRedis连接 1.导入依赖 12345678910&lt;!--Redis依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--连接池依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 2.配置连接信息 123456789101112spring: redis: host: 192.168.200.130 port: 6379 password: 1234 database: 0 lettuce: pool: max-active: 8 #最大连接数 max-idle: 8 #最大空闲连接 min-idle: 0 #最小空闲连接 max-wait: 100 #连接等待时间 3.直接注入RedisTemplate出现的问题 123456789101112131415161718192021222324252627// 自动注入的 `RedisTemplate` 需要加上泛型@Resourceprivate RedisTemplate redisTemplate;@Testpublic void test() { redisTemplate.opsForValue().set(&quot;k1&quot;, &quot;v1&quot;); Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(&quot;k2&quot;, &quot;v2&quot;); map.put(&quot;k3&quot;, &quot;v3&quot;); map.put(&quot;k4&quot;, &quot;v4&quot;); map.put(&quot;k5&quot;, &quot;v5&quot;); redisTemplate.opsForValue().multiSet(map); redisTemplate.opsForValue().multiGet(Arrays.asList(&quot;k1&quot;, &quot;k2&quot;, &quot;k3&quot;, &quot;k4&quot;)).forEach(System.out::println); // v1 v2 v3 v4 v5}//结果# 在 Redis 中查看通过 RedisTemplate 插入的数据&gt; keys *1) &quot;\\xac\\xed\\x00\\x05t\\x00\\x02k1&quot;2) &quot;\\xac\\xed\\x00\\x05t\\x00\\x02k2&quot;3) &quot;\\xac\\xed\\x00\\x05t\\x00\\x02k3&quot;4) &quot;\\xac\\xed\\x00\\x05t\\x00\\x02k4&quot;5) &quot;\\xac\\xed\\x00\\x05t\\x00\\x02k5&quot;&gt; get &quot;\\xac\\xed\\x00\\x05t\\x00\\x02k1&quot;&quot;\\xac\\xed\\x00\\x05t\\x00\\x02v1&quot; RedisTemplate 存在的问题 通过以上操作可以发现：RedisTemplate 可以将任意类型的数据写入到 Redis 中，在写入前会将其序列化为字节形式存储，底层默认采用 ObjectOutputStream 序列化。 4.因此我们要重写他的序列化工具 导入 jackson-databind 依赖，并编写配置类 RedisTemplateConfig。 12345678910111213141516171819202122@Configurationpublic class RedisTemplateConfig { @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { // 创建 RedisTemplate 对象 RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); // 设置连接工厂 redisTemplate.setConnectionFactory(redisConnectionFactory); // 设置序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // Key 和 HashKey 采用 String 序列化（StringRedisSerializer） redisTemplate.setKeySerializer(RedisSerializer.string()); redisTemplate.setHashKeySerializer(RedisSerializer.string()); // Value 和 HashValue 采用 JSON 序列化（GenericJackson2JsonRedisSerializer） redisTemplate.setValueSerializer(jsonRedisSerializer); redisTemplate.setHashValueSerializer(jsonRedisSerializer); return redisTemplate; }} 12345678910// 自动注入的 `RedisTemplate` 需要加上泛型@Autowiredprivate RedisTemplate&lt;String, Object&gt; redisTemplate;@Testpublic void test() { redisTemplate.opsForValue().set(&quot;k1&quot;, &quot;v1&quot;); redisTemplate.opsForValue().set(&quot;user:1&quot;, new User(&quot;Jack&quot;, 21));} 通过以上的方法能够解决数据序列化时 可读性差、内存占用大 的问题。 但是 JSON 的序列化方式仍然存在一些问题：为了反序列化时知道对象的类型，JSON 序列化器会将类的 class 类型写入 JSON 结果，存入 Redis 中，会带来额外的内存开销。 5.使用StringRedisTemplate 为了节省内存空间，Spring 提供了一个 StringRedisTemplate，它的 key 和 value 的序列化方式默认就是 String，统一使用 String 序列化器。 当需要存储 Java 对象时，手动完成对象的序列化和反序列化。 使用 StringRedisTemplate。 写入数据到 Redis 中，手动将对象序列化为 JSON。 从 Redis 中读取数据，手动将读取到的 JSON 反序列化为对象。 123456789101112131415161718192021222324@Autowiredprivate StringRedisTemplate stringRedisTemplate;private static final ObjectMapper objectMapper = new ObjectMapper();@Testpublic void ttt() throws JsonProcessingException { User user = new User(&quot;Michael&quot;, 27); // 手动序列化 String json = objectMapper.writeValueAsString(user); // 写入数据 stringRedisTemplate.opsForValue().set(&quot;user:1&quot;, json); // 读取数据 String data = stringRedisTemplate.opsForValue().get(&quot;user:1&quot;); // 反序列化 User deserializedUser = objectMapper.readValue(data, User.class); System.out.println(deserializedUser);}//结果{ &quot;username&quot;: &quot;Michael&quot;, &quot;age&quot;: 27}","link":"/posts/redis%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84/"},{"title":"redis缓存的应用","text":"简单的缓存策略 12345678910111213141516171819202122232425262728@Servicepublic class ShopServiceImpl extends ServiceImpl&lt;ShopMapper, Shop&gt; implements IShopService { @Resource private StringRedisTemplate stringRedisTemplate; @Override public Result querygetById(Long id) { //1.从Redis内查询商品缓存 String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY + id); if(StrUtil.isNotBlank(shopJson)){ //手动反序列化 Shop shop = JSONUtil.toBean(shopJson, Shop.class); return Result.ok(shop); } //2.不存在就根据id查询数据库 Shop shop = getById(id); if(shop==null){ return Result.fail(&quot;商户不存在！&quot;); } //3.数据库数据写入Redis //手动序列化 String shopStr = JSONUtil.toJsonStr(shop); stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id,shopStr,CACHE_SHOP_TTL, TimeUnit.MINUTES); return Result.ok(shop); }} 缓存更新策略 缓存更新策略的最佳方案： 低一致性需求：使用Redis自带的内存淘汰机制 高一致性需求：主动更新，超时剔除的方式作为斗地方案 读操作 缓存命中就直接返回 缓存未命中则查询数据库 写操作： 先写数据库，然后再删除缓存 要确保数据库与缓存操作的原子性 操作缓存和数据库时需要考虑的三个问题 删除缓存还是更新缓存？ 每次更新数据库的同时更新缓存：若数据库更新了 100 次，期间没有任何查询请求，此时缓存的更新就是无效操作。 数据库更新就删除缓存：数据库更新后缓存被删除，此时数据库无论更新多少次，缓存都不会做任何操作。直到有查询请求，缓存才会将数据库中的数据写入到缓存中。 如何保证缓存和数据库的操作同时成功或失败？ 单体系统：将缓存与数据库操作放在一个事务。 分布式系统：利用 TCC 等分布式事务方案。 先操作缓存还是先操作数据库？ 先删除缓存，再操作数据库。假设缓存为 10，数据库为 10。（t1、t2、t3 代表三个时刻） t1：线程 1 删除缓存，并更新数据库为 20。t2：线程 2 查询缓存未命中，从数据库中查询并写入缓存。✔️ t1：线程 1 删除缓存。t2：线程 2 查询缓存未命中，从数据库中查询并写入缓存。t3：线程 t1 更新数据库为 20。❌ **先操作数据库，再删除缓存。**假设缓存为 10，数据库为 10。（t1、t2、t3、t4 代表四个时刻） t1：线程 1 更新数据库为 20，删除缓存。t2：线程 2 查询缓存未命中，从数据库中查询并写入缓存。✔️ t1：线程 1 查询缓存未命中，从数据库中查询。t2：线程 2 更新数据库为 20，删除缓存。t3：线程 1 写入缓存。❌（这种方式出现的概率很小，缓存写入的速度很快。更可能出现的情况是：线程 1 写入缓存后，线程 2 更新数据库然后将缓存删除） 123456789101112@Overridepublic Result update(Shop shop) { if(shop.getId()==null){ return Result.fail(&quot;店铺id不能为空!&quot;); } //1.更新数据库 updateById(shop); //2.删除缓存 String key = CACHE_SHOP_KEY + shop.getId(); stringRedisTemplate.delete(key); return Result.ok();} 缓存穿透问题 缓存空对象方案 客户端请求的数据在 Redis 和数据库中都不存在，为了防止不断的请求：将 空值 缓存到 Redis 中并且设置 TTL 时间后，返回给该请求。 缓存中包含过多的 空值，会造成额外的内存消耗。（设置 TTL 可以缓解） 可能造成短期的不一致：第一次请求的数据在 Redis 和数据库中都不存在，缓存空对象后，数据库中新增了该请求对应的数据 1234567891011121314151617181920212223242526272829@Overridepublic CommonResult&lt;Shop&gt; getShopById(Long id) { ThrowUtils.throwIf(id == null, ErrorCode.PARAMS_ERROR); String shopKey = CACHE_SHOP_KEY + id; // 1. 先从 Redis 中查询数据，存在则将其转换为 Java 对象后返回 String shopJsonInRedis = stringRedisTemplate.opsForValue().get(shopKey); if (StringUtils.isNotBlank(shopJsonInRedis)) { return CommonResult.success(JSONUtil.toBean(shopJsonInRedis, Shop.class)); } // 命中空值 if (shopJsonInRedis != null) { throw new BusinessException(ErrorCode.NOT_FOUND_ERROR, &quot;该商铺不存在&quot;); } // 2. 从 Redis 中未查询到数据，则从数据库中查询 Shop shop = this.getById(id); // 若数据中也查询不到，则缓存空值后返回提示信息 if (shop == null) { stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, &quot;&quot;, TTL_TWO, TimeUnit.MINUTES); throw new BusinessException(ErrorCode.NOT_FOUND_ERROR, &quot;该商铺不存在&quot;); } // 3. 将从数据库中查询到的数据存入 Redis 后返回 stringRedisTemplate.opsForValue().set(shopKey, JSONUtil.toJsonStr(shop), TTL_TWO, TimeUnit.HOURS); return CommonResult.success(shop);} 布隆过滤器方案 布隆过滤器（Bloom Filter）：一个很长的二进制数组（初始化值为 0），通过一系列的 Hash 函数判断该数据是否存在。 布隆过滤器的运行速度快、内存占用小，但是存在误判的可能。 存储数据时经过 n 个 hash 函数，计算出 n 个 hash 值，hash 值映射后得到 n 个索引，设置索引处的值为 1。（若当前索引处值已经为 1，则不需要任何操作） 查询数据时也会经过 n 个 hash 函数，计算出 n 个 hash 值，hash 值映射后得到 n 个索引，判断索引处的值是否为 1。 查询 Anthony：经过 hash 算法得到的 hash 值映射后数组下标为 0、2、6，下标对应的值没有全为 1，数组中不存在该元素。 查询 Coco：经过 hash 算法得到的 hash 值映射后数组下标为 0、2、6，下标对应的值都为 1，数组中可能存在该元素。 缓存雪崩问题： 解决方案： 给不同的Key的TTL添加随机值 利用Redis集群提高服务的可用性 给缓存业务添加降级限流策略 给业务添加多级缓存 缓存击穿问题 缓存击穿问题，也叫 热点 Key 问题；就是一个被 高并发访问 并且 缓存中业务较复杂的 Key 突然失效，大量的请求在极短的时间内一起请求这个 Key 并且都未命中，无数的请求访问在瞬间打到数据库上，给数据库带来巨大的冲击。 缓存击穿整体过程： 一个线程查询缓存，未命中，查询数据库并重建缓存（缓存重建业务比较复杂，时间长）。 在这个重建缓存的过程中，大量的请求穿过缓存直接请求数据库并重建缓存，导致性能下降。 解决方案：互斥锁(一致性)、逻辑过期(可用性) 互斥锁方案 synchronized 查询缓存，存在则直接返回。 不存在：执行 synchronized 代码块。 先查缓存，存在则直接返回。（若多个线程执行到同步代码块，某个线程拿到锁查询数据库并重建缓存后，其他拿到锁进来的线程直接查询缓存后返回，避免重复查询数据库并重建缓存） 查询数据库，重建缓存。 12345678910111213141516171819202122232425262728293031323334353637@SneakyThrows@Overridepublic CommonResult&lt;Shop&gt; getShopById(Long id) { ThrowUtils.throwIf(id == null, ErrorCode.PARAMS_ERROR); String shopKey = CACHE_SHOP_KEY + id; // 1. 先从 Redis 中查询数据，存在则将其转换为 Java 对象后返回 String shopJsonInRedis = stringRedisTemplate.opsForValue().get(shopKey); if (StringUtils.isNotBlank(shopJsonInRedis)) { return CommonResult.success(JSONUtil.toBean(shopJsonInRedis, Shop.class)); } // 命中空值 if (shopJsonInRedis != null) { throw new BusinessException(ErrorCode.NOT_FOUND_ERROR, &quot;该商铺不存在&quot;); } // 2. 从 Redis 中未查询到数据，则从数据库中查询。（synchronized） Shop shop = new Shop(); synchronized (ShopServiceImpl.class) { // 3. 再次查询 Redis：若多个线程执行到同步代码块，某个线程拿到锁查询数据库并重建缓存后，其他拿到锁进来的线程直接查询缓存后返回，避免重复查询数据库并重建缓存。 shopJsonInRedis = stringRedisTemplate.opsForValue().get(shopKey); if (StringUtils.isNotBlank(shopJsonInRedis)) { return CommonResult.success(JSONUtil.toBean(shopJsonInRedis, Shop.class)); } // 4. 查询数据库，缓存空值避免缓存穿透，重建缓存。 shop = this.getById(id); if (shop == null) { stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, &quot;&quot;, TTL_TWO, TimeUnit.MINUTES); throw new BusinessException(ErrorCode.NOT_FOUND_ERROR, &quot;该商铺不存在&quot;); } // 模拟缓存重建延迟 Thread.sleep(100); stringRedisTemplate.opsForValue().set(shopKey, JSONUtil.toJsonStr(shop), TTL_TWO, TimeUnit.HOURS); } return CommonResult.success(shop);} 用redis的setnx来充当分布式锁 1234567891011121314/** * 获取互斥锁 */public boolean tryLock(String key) { Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, &quot;1&quot;, TTL_TWO, TimeUnit.SECONDS); return Boolean.TRUE.equals(result);}/** * 释放互斥锁 */public void unlock(String key) { stringRedisTemplate.delete(key);} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@SneakyThrows@Overridepublic CommonResult&lt;Shop&gt; getShopById(Long id) { ThrowUtils.throwIf(id == null, ErrorCode.PARAMS_ERROR); String shopKey = CACHE_SHOP_KEY + id; String lockKey = LOCK_SHOP_KEY + id; // 1. 先从 Redis 中查询数据，存在则将其转换为 Java 对象后返回 String shopJsonInRedis = stringRedisTemplate.opsForValue().get(shopKey); if (StringUtils.isNotBlank(shopJsonInRedis)) { return CommonResult.success(JSONUtil.toBean(shopJsonInRedis, Shop.class)); } // 命中空值 if (shopJsonInRedis != null) { throw new BusinessException(ErrorCode.NOT_FOUND_ERROR, &quot;该商铺不存在&quot;); } // 2. 从 Redis 中未查询到数据，尝试获取锁后从数据库中查询。 Shop shop = new Shop(); boolean tryLock = tryLock(lockKey); try { // 2.1 未获取到锁则等待一段时间后重试（通过递归调用重试） if (BooleanUtil.isFalse(tryLock)) { Thread.sleep(50); this.getShopById(id); } // 2.2 获取到锁：查询数据库、缓存重建。 if (tryLock) { // 3. 再次查询 Redis：若多个线程执行到获取锁处，某个线程拿到锁查询数据库并重建缓存后，其他拿到锁进来的线程直接查询缓存后返回，避免重复查询数据库并重建缓存。 shopJsonInRedis = stringRedisTemplate.opsForValue().get(shopKey); if (StringUtils.isNotBlank(shopJsonInRedis)) { return CommonResult.success(JSONUtil.toBean(shopJsonInRedis, Shop.class)); } // 4. 查询数据库，缓存空值避免缓存穿透，重建缓存。 shop = this.getById(id); if (shop == null) { stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, &quot;&quot;, TTL_TWO, TimeUnit.MINUTES); throw new BusinessException(ErrorCode.NOT_FOUND_ERROR, &quot;该商铺不存在&quot;); } // 模拟缓存重建延迟 Thread.sleep(100); stringRedisTemplate.opsForValue().set(shopKey, JSONUtil.toJsonStr(shop), TTL_TWO, TimeUnit.HOURS); } } finally { // 5. 释放锁 unlock(lockKey); } return CommonResult.success(shop);} 逻辑过期方案 无需考虑缓存雪崩（Redis 宕机除外）、缓存穿透问题：缓存何时过期通过代码控制而非 TTL。需要进行数据预热，缓存未命中时直接返回空。 先查询缓存，未命中则直接返回。 命中则判断缓存是否过期，未过期则直接返回。 过期：获取锁。 未获取到锁：直接返回。 获取到锁：开启一个新的线程后直接返回，这个线程负责重建缓存后释放锁。 存储到 Redis 中的 Key 永久有效，过期时间通过代码控制而非 TTL。Redis 存储的数据需要带上一个逻辑过期时间，即 Shop 实体类中需要一个逻辑过期时间属性。新建一个 RedisData，该类包含两个属性 expireTime 和 Data，对原来的代码没有入侵性。 缓存预热（将热点数据提前存储到 Redis 中） 123456@Datapublic class RedisData { private LocalDateTime expireTime; private Object data;} 123456789101112/** * 缓存预热（将热点数据提前存储到 Redis 中） */public void saveHotDataIn2Redis(Long id, Long expireSeconds) { Shop shop = this.getById(id); ThrowUtils.throwIf(shop == null, ErrorCode.NOT_FOUND_ERROR, &quot;该数据不存在&quot;); RedisData redisData = new RedisData(); redisData.setData(shop); redisData.setExpireTime(LocalDateTime.now().plusSeconds(expireSeconds)); stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(redisData));} 1234567891011# Redis 中存储的数据会多一个 expireTime 的值{ &quot;expireTime&quot;: 1681660099861, &quot;data&quot;: { &quot;id&quot;: 1, &quot;name&quot;: &quot;101茶餐厅&quot;, &quot;typeId&quot;: 1, ... }} 逻辑过期 1234567891011121314/** * 缓存预热（将热点数据提前存储到 Redis 中） */public void saveHotDataIn2Redis(Long id, Long expireSeconds) throws InterruptedException { Shop shop = this.getById(id); ThrowUtils.throwIf(shop == null, ErrorCode.NOT_FOUND_ERROR, &quot;该数据不存在&quot;); // 模拟缓存重建延迟，让一部分线程先执行完毕，在此期间会短暂的不一致 Thread.sleep(200); RedisData redisData = new RedisData(); redisData.setData(shop); redisData.setExpireTime(LocalDateTime.now().plusSeconds(expireSeconds)); stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(redisData));} 1234567891011121314151617181920212223242526272829303132333435363738394041private static final ExecutorService ES = Executors.newFixedThreadPool(10);@SneakyThrows@Overridepublic CommonResult&lt;Shop&gt; getShopById(Long id) { ThrowUtils.throwIf(id == null, ErrorCode.PARAMS_ERROR); String shopKey = CACHE_SHOP_KEY + id; String lockKey = LOCK_SHOP_KEY + id; // 1. 先从 Redis 中查询数据，未命中则直接返回 String redisDataJson = stringRedisTemplate.opsForValue().get(shopKey); if (StringUtils.isBlank(redisDataJson)) { return CommonResult.success(null); } // 2. 判断是否过期，未过期则直接返回 RedisData redisData = JSONUtil.toBean(redisDataJson, RedisData.class); JSONObject jsonObject = (JSONObject) redisData.getData(); Shop shop = JSONUtil.toBean(jsonObject, Shop.class); LocalDateTime expireTime = redisData.getExpireTime(); if (expireTime.isAfter(LocalDateTime.now())) { return CommonResult.success(shop); } // 3. 未获取到锁直接返回 boolean tryLock = tryLock(lockKey); if (BooleanUtil.isFalse(tryLock)) { return CommonResult.success(shop); } // 4. 获取到锁：开启一个新的线程后返回旧数据。（这个线程负责查询数据库、重建缓存） // 此处无需 DoubleCheck，因为未获取到锁直接返回旧数据，能保证只有一个线程执行到此处 ES.submit(() -&gt; { try { // 查询数据库、重建缓存 this.saveHotDataIn2Redis(id, 3600 * 24L); } catch (Exception e) { log.error(e.getMessage()); } finally { unlock(lockKey); } }); return CommonResult.success(shop);} 总结Redis Cache工具类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219@Component@Slf4jpublic class CacheClient { private static final ExecutorService ES = Executors.newFixedThreadPool(10); private final StringRedisTemplate stringRedisTemplate; public CacheClient(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } /** * 获取锁 */ public boolean tryLock(String key) { Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(key, &quot;1&quot;, TTL_TWO, TimeUnit.SECONDS); return BooleanUtil.isTrue(result); } /** * 释放锁 */ public void unlock(String key) { stringRedisTemplate.delete(key); } /** * 数据预热（将热点数据提前存储到 Redis 中） * * @param key 预热数据的 Key * @param value 预热数据的 Value * @param expireTime 逻辑过期时间 * @param timeUnit 时间单位 */ public void dataWarmUp(String key, Object value, Long expireTime, TimeUnit timeUnit) { RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(timeUnit.toSeconds(expireTime))); stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData)); } /** * 将 Java 对象序列化为 JSON 存储到 Redis 中并且设置 TTL 过期时间 * * @param key String 类型的键 * @param value 序列化为 JSON 的值 * @param time TTL 过期时间 * @param timeUnit 时间单位 */ public void set(String key, Object value, Long time, TimeUnit timeUnit) { stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, timeUnit); } /** * 解决缓存穿透问（缓存空值） * * @param keyPrefix Key 前缀 * @param id id * @param type 实体类型 * @param function 有参有返回值的函数 * @param time TTL 过期时间 * @param timeUnit 时间单位 * @param &lt;R&gt; 实体类型 * @param &lt;ID&gt; id 类型 * @return 设置某个实体类的缓存，并解决缓存穿透问题 */ public &lt;R, ID&gt; R setWithCachePenetration(String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; function, Long time, TimeUnit timeUnit) { String key = keyPrefix + id; // 1. 先从 Redis 中查询数据，存在则将其转换为 Java 对象后返回 String jsonStr = stringRedisTemplate.opsForValue().get(key); if (StringUtils.isNotBlank(jsonStr)) { return JSONUtil.toBean(jsonStr, type); } // 命中空值 if (jsonStr != null) { throw new BusinessException(ErrorCode.NOT_FOUND_ERROR); } // 2. 从 Redis 中未查询到数据，则从数据库中查询 R result = function.apply(id); // 若数据中也查询不到，则缓存空值后返回提示信息 if (result == null) { stringRedisTemplate.opsForValue().set(key, &quot;&quot;, TTL_TWO, TimeUnit.MINUTES); throw new BusinessException(ErrorCode.NOT_FOUND_ERROR); } // 3. 将从数据库中查询到的数据存入 Redis 后返回 this.set(key, result, time, timeUnit); return result; } /** * 解决缓存击穿问题（synchronized） */ public &lt;R, ID&gt; R setWithCacheBreakdown4Synchronized(String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; function, Long time, TimeUnit timeUnit) { String key = keyPrefix + id; // 1. 先从 Redis 中查询数据，存在则将其转换为 Java 对象后返回 String jsonStr = stringRedisTemplate.opsForValue().get(key); if (StringUtils.isNotBlank(jsonStr)) { return JSONUtil.toBean(jsonStr, type); } // 命中空值 if (jsonStr != null) { throw new BusinessException(ErrorCode.NOT_FOUND_ERROR); } // 2. 从 Redis 中未查询到数据，则从数据库中查询。（synchronized） R result = null; synchronized (CacheClient.class) { // 3. 再次查询 Redis：若多个线程执行到同步代码块，某个线程拿到锁查询数据库并重建缓存后，其他拿到锁进来的线程直接查询缓存后返回，避免重复查询数据库并重建缓存。 jsonStr = stringRedisTemplate.opsForValue().get(key); if (StringUtils.isNotBlank(jsonStr)) { return JSONUtil.toBean(jsonStr, type); } // 4. 查询数据库、缓存空值避免缓存穿透、重建缓存。 result = function.apply(id); if (result == null) { stringRedisTemplate.opsForValue().set(key, &quot;&quot;, TTL_TWO, TimeUnit.MINUTES); throw new BusinessException(ErrorCode.NOT_FOUND_ERROR); } this.set(key, result, time, timeUnit); } return result; } /** * 解决缓存击穿问题（setnx） */ public &lt;R, ID&gt; R setWithCacheBreakdown4SetNx(String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; function, Long time, TimeUnit timeUnit) { String key = keyPrefix + id; String lockKey = LOCK_SHOP_KEY + id; // 1. 先从 Redis 中查询数据，存在则将其转换为 Java 对象后返回 String jsonStr = stringRedisTemplate.opsForValue().get(key); if (StringUtils.isNotBlank(jsonStr)) { return JSONUtil.toBean(jsonStr, type); } // 命中空值 if (jsonStr != null) { throw new BusinessException(ErrorCode.NOT_FOUND_ERROR); } // 2. 从 Redis 中未查询到数据，尝试获取锁后从数据库中查询。 R result = null; boolean tryLock = tryLock(lockKey); try { // 2.1 未获取到锁则等待一段时间后重试（通过递归调用重试） if (BooleanUtil.isFalse(tryLock)) { Thread.sleep(50); this.setWithCacheBreakdown4SetNx(keyPrefix, id, type, function, time, timeUnit); } // 2.2 获取到锁：查询数据库、缓存重建。 if (tryLock) { // 3. 再次查询 Redis：若多个线程执行到同步代码块，某个线程拿到锁查询数据库并重建缓存后，其他拿到锁进来的线程直接查询缓存后返回，避免重复查询数据库并重建缓存。 jsonStr = stringRedisTemplate.opsForValue().get(key); if (StringUtils.isNotBlank(jsonStr)) { return JSONUtil.toBean(jsonStr, type); } // 4. 查询数据库、缓存空值避免缓存穿透、重建缓存。 result = function.apply(id); if (result == null) { stringRedisTemplate.opsForValue().set(key, &quot;&quot;, TTL_TWO, TimeUnit.MINUTES); throw new BusinessException(ErrorCode.NOT_FOUND_ERROR); } this.set(key, result, time, timeUnit); } } catch (Exception e) { log.error(e.getMessage()); } finally { unlock(lockKey); } return result; } /** * 解决缓存击穿问题（逻辑过期时间） */ public &lt;R, ID&gt; R setWithCacheBreakdown4LogicalExpiration(String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; function, Long time, TimeUnit timeUnit) { String key = keyPrefix + id; String lockKey = LOCK_SHOP_KEY + id; // 1. 先从 Redis 中查询数据，未命中则直接返回 String jsonStr = stringRedisTemplate.opsForValue().get(key); if (StringUtils.isBlank(jsonStr)) { return null; } // 2. 判断是否过期，未过期则直接返回 RedisData redisData = JSONUtil.toBean(jsonStr, RedisData.class); JSONObject jsonObject = JSONUtil.parseObj(redisData.getData()); R result = JSONUtil.toBean(jsonObject, type); LocalDateTime expireTime = redisData.getExpireTime(); if (expireTime.isAfter(LocalDateTime.now())) { return result; } // 3. 未获取到锁直接返回 boolean tryLock = tryLock(lockKey); if (BooleanUtil.isFalse(tryLock)) { return result; } // 4. 获取到锁：开启一个新的线程后返回旧数据。（这个线程负责查询数据库、重建缓存） // 此处无需 DoubleCheck，因为未获取到锁直接返回旧数据，能保证只有一个线程执行到此处 ES.submit(() -&gt; { try { this.dataWarmUp(key, function.apply(id), time, timeUnit); } finally { unlock(lockKey); } }); return result; }}","link":"/posts/redis%E7%BC%93%E5%AD%98%E7%9A%84%E5%BA%94%E7%94%A8/"},{"title":"redis解决常见的秒杀问题","text":"秒杀问题 每个店铺都可以发布优惠券，保存到 tb_voucher 表中；当用户抢购时，生成订单并保存到 tb_voucher_order 表中。 订单表如果使用数据库自增 ID，会存在以下问题： ID 的规律太明显，容易暴露信息。 单表数据量的限制，订单过多时单表很难存储得下。数据量过大后需要拆库拆表，但拆分表了之后，各表从逻辑上是同一张表，所以 id 不能一样， 于是需要保证 ID 的唯一性。 全局唯一ID 全局唯一 ID 的特点 唯一性：Redis 独立于数据库之外，不论有多少个数据库、多少张表，访问 Redis 获取到的 ID 可以保证唯一。 高可用：Redis 高可用（集群等方案）。 高性能：Redis 速度很快。 递增性：例如 String 的 INCR 命令，可以保证递增。 安全性：为了增加 ID 的安全性，在使用 Redis 自增数值的基础上，在拼接一些其他信息。 全局唯一 ID 的组成（存储数值类型占用空间更小，使用 long 存储，8 byte，64 bit） 符号位：1 bit，永远为 0，代表 ID 是正数。 时间戳：31 bit，以秒为单位，可以使用 69 年。 序列号：32 bit，当前时间戳对应的数量，也就是每秒可以对应 2^32 个不同的 ID。 Redis ID 自增策略：通过设置每天存入一个 Key，方便统计订单数量；ID 构造为 时间戳 + 计数器。 123456789101112131415161718192021222324252627@Componentpublic class RedisIdWorker { /** * 指定时间戳（2023年1月1日 0:0:00） LocalDateTime.of(2023, 1, 1, 0, 0, 0).toEpochSecond(ZoneOffset.UTC) */ private static final long BEGIN_TIMESTAMP_2023 = 1672531200L; /** * 序列号位数 */ private static final int BIT_COUNT = 32; private final StringRedisTemplate stringRedisTemplate; public RedisIdWorker(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public long nextId(String keyPrefix) { // 1. 时间戳 long timestamp = LocalDateTime.now().toEpochSecond(ZoneOffset.UTC) - BEGIN_TIMESTAMP_2023; // 2. 生成序列号：自增 1，Key 不存在会自动创建一个 Key。（存储到 Redis 中的 Key 为 keyPrefix:date，Value 为自增的数量） Long serialNumber = stringRedisTemplate.opsForValue().increment(keyPrefix + &quot;:&quot; + DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd&quot;).format(LocalDate.now())); // 3. 时间戳左移 32 位，序列号与右边的 32 个 0 进行与运算 return timestamp &lt;&lt; BIT_COUNT | serialNumber; }} 测试(300个线程生成共3w个id) 1234567891011121314151617181920212223242526@Resourceprivate RedisIdWorker redisIdWorker;public static final ExecutorService ES = Executors.newFixedThreadPool(500);@Testvoid testGloballyUniqueID() throws Exception { // 程序是异步的，分线程全部走完之后主线程再走，使用 CountDownLatch；否则异步程序没有执行完时主线程就已经执行完了 CountDownLatch latch = new CountDownLatch(300); Runnable task = () -&gt; { for (int i = 0; i &lt; 100; i++) { long globallyUniqueID = redisIdWorker.nextId(&quot;sun&quot;); System.out.println(&quot;globallyUniqueID = &quot; + globallyUniqueID); } latch.countDown(); }; long begin = System.currentTimeMillis(); for (int i = 0; i &lt; 300; i++) { ES.submit(task); } latch.await(); long end = System.currentTimeMillis(); System.out.println(&quot;Execution Time: &quot; + (end - begin));} 添加优惠卷 格式类似这种逻辑太简单了略 123456789101112{ &quot;shopId&quot;:1, &quot;title&quot;:&quot;100元代金券&quot;, &quot;subTitle&quot;:&quot;周一至周五均可使用&quot;, &quot;rules&quot;:&quot;全场通用\\n无需预约\\n可无限叠加\\n不兑现、不找零\\n仅限堂食&quot;, &quot;payValue&quot;:8000, &quot;actualValue&quot;:10000, &quot;type&quot;:1, &quot;stock&quot;:100, &quot;beginTime&quot;:&quot;2022-11-13T10:09:17&quot;, &quot;endTime&quot;:&quot;2022-11-13T22:10:17&quot;} 秒杀下单功能 1234567891011121314151617181920212223242526272829303132333435363738394041@Override@Transactionalpublic Result seckillVoucher(Long voucherId) { //1.查询优惠卷 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); //2.判断秒杀是否开始，是否结束 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { return Result.fail(&quot;秒杀尚未开始!&quot;); } if(voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(&quot;秒杀已结束!&quot;); } //3.判断库存是否充足 if(voucher.getStock()&lt;=0){ return Result.fail(&quot;优惠券库存不足!&quot;); } //4.扣减库存 boolean success = seckillVoucherService.update() .setSql(&quot;stock = stock -1&quot;) .eq(&quot;voucher_id&quot;, voucherId).update(); //5.创建订单 if(!success){ return Result.fail(&quot;优惠券库存不足!&quot;); } //6.返回订单id VoucherOrder voucherOrder = new VoucherOrder(); //6.1订单id long orderId = redisIdWorker.nextId(&quot;order&quot;); voucherOrder.setId(orderId); //6.2用户id Long userId = UserHolder.getUser().getId(); voucherOrder.setUserId(userId); //6.3代金券id voucherOrder.setVoucherId(voucherId); //7.订单写入数据库 save(voucherOrder); //8.返回订单Id return Result.ok(orderId);} 超卖问题 假设库存为 1，有线程1、2、3，时刻 t1、t2、t3、t4。 t1：线程1 查询库存，库存为 1； t2：线程2、线程 3 查询库存，库存为 1； t3：线程1 下单，库存扣减为 0。 t4：线程2 和 线程3 下单，库存扣减为 -2。 具体图示: 解决超卖问题 悲观锁 太简单了直接加锁保证操作数据是原子操作要串行执行 乐观锁 版本号法: 一般是在数据库表中加上一个 version 字段表示 数据被修改的次数。数据被修改时 version 值加 1。 线程 A 读取数据，同时读取到 version 值。 提交更新时，若刚才读到的 version 值未发生变化：则提交更新并且 version 值加 1。 提交更新时，若刚才读到的 version 值发生了变化：放弃更新，并通过报错、自旋重试等方式进行下一步处理。 CAS法(简单来说就是直接拿库存当版本号): CAS 操作需要输入两个数值，一个旧值（操作前的值）和一个新值，操作时先比较下在旧值有没有发生变化，若未发生变化才交换成新值，发生了变化则不交换。 CAS 是原子操作，多线程并发使用 CAS 更新数据时，可以不使用锁。原子操作是最小的不可拆分的操作，操作一旦开始，不能被打断，直到操作完成。也就是多个线程对同一块内存的操作是串行的。 一人一单问题 一人一单逻辑: 发送下单请求，提交优惠券 ID。 下单前需要判断：秒杀是否开始或结束、库存是否充足。 库存充足：根据优惠券 ID 和用户 ID 查询订单，判断该用户是否购买过该优惠券。 该用户对该优惠券的订单不存在时，扣减库存、创建订单、返回订单 ID。 解决并发安全问题 单人下单（一个用户），高并发的情况下：该用户的 10 个线程同时执行到 查询该用户 ID 和秒杀券对应的订单数量，10 个线程查询到的值都为 0，即未下单。于是会出现一个用户下 10 单的情况。 **此处仍需加锁，乐观锁适合更新操作，插入操作需要选择悲观锁。**若直接在方法上添加 synchronized 关键字，会让锁的范围（粒度）过大，导致性能较差。因此，采用 一个用户一把锁 的方式。 问题：能否用乐观锁执行？ 不能，原因是乐观锁只能操作(修改)单个变量，而创建订单需要操作数据库(难以跟踪状态) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Overridepublic CommonResult&lt;Long&gt; seckillVoucher(Long voucherId) { // 判断秒杀是否开始或结束、库存是否充足。 SeckillVoucher seckillVoucher = seckillVoucherService.getById(voucherId); ThrowUtils.throwIf(seckillVoucher == null, ErrorCode.NOT_FOUND_ERROR); LocalDateTime now = LocalDateTime.now(); ThrowUtils.throwIf(now.isBefore(seckillVoucher.getBeginTime()), ErrorCode.OPERATION_ERROR, &quot;秒杀尚未开始&quot;); ThrowUtils.throwIf(now.isAfter(seckillVoucher.getEndTime()), ErrorCode.OPERATION_ERROR, &quot;秒杀已经结束&quot;); ThrowUtils.throwIf(seckillVoucher.getStock() &lt; 1, ErrorCode.OPERATION_ERROR, &quot;库存不足&quot;); // 下单 return this.createVoucherOrder(voucherId);}/** * 下单（超卖 - CAS、一人一单 - synchronized） */@Override@Transactionalpublic CommonResult&lt;Long&gt; createVoucherOrder(Long voucherId) { // 1. 判断当前用户是否下过单 Long userId = UserHolder.getUser().getId(); Integer count = this.lambdaQuery() .eq(VoucherOrder::getVoucherId, voucherId) .eq(VoucherOrder::getUserId, userId) .count(); ThrowUtils.throwIf(count &gt; 0, ErrorCode.OPERATION_ERROR, &quot;禁止重复下单&quot;); // 2. 扣减库存 boolean result = seckillVoucherService.update() .setSql(&quot;stock = stock - 1&quot;) .eq(&quot;voucher_id&quot;, voucherId) .gt(&quot;stock&quot;, 0) .update(); ThrowUtils.throwIf(!result, ErrorCode.OPERATION_ERROR, &quot;下单失败&quot;); // 3. 下单 VoucherOrder voucherOrder = new VoucherOrder(); voucherOrder.setUserId(userId); voucherOrder.setId(redisIdWorker.nextId(&quot;seckillVoucherOrder&quot;)); voucherOrder.setVoucherId(voucherId); result = this.save(voucherOrder); ThrowUtils.throwIf(!result, ErrorCode.OPERATION_ERROR, &quot;下单失败&quot;); return CommonResult.success(voucherOrder.getId());} 集群环境下的并发问题 分布式锁-原理 不去使用jvm内部的锁监视器，我们要在外部开一个锁监视器，让它监视所有的线程 常见的分布式锁 MySQL：MySQL 本身带有锁机制，但是由于 MySQL 性能一般，所以采用分布式锁的情况下，使用 MySQL 作为分布式锁比较少见。 Redis：Redis 作为分布式锁比较常见，利用 setnx 方法，如果 Key 插入成功，则表示获取到锁，插入失败则表示无法获取到锁。 Zookeeper：Zookeeper 也是企业级开发中比较好的一个实现分布式锁的方案。 MySQL Redis Zookeeper 互斥 利用 MySQL 本身的互斥锁机制 利用 setnx 互斥命令 利用节点的唯一性和有序性 高可用 好 好 好 高性能 一般 好 一般 安全性 断开链接，自动释放锁 利用锁超时时间，到期释放 临时节点，断开链接自动释放 12345# 添加锁（NX 互斥、EX 设置 TTL 时间）SET lock thread1 NX EX 10# 手动释放锁DEL lock 123456789101112131415161718192021222324252627282930313233343536373839public interface DistributedLock { /** * 获取锁（只有一个线程能够获取到锁） * @param timeout 锁的超时时间，过期后自动释放 * @return true 代表获取锁成功；false 代表获取锁失败 */ boolean tryLock(long timeout); /** * 释放锁 */ void unlock();}public class SimpleDistributedLock4Redis implements DistributedLock { private static final String KEY_PREFIX = &quot;lock:&quot;; private final String name; private final StringRedisTemplate stringRedisTemplate; public SimpleDistributedLockBased4Redis(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean tryLock(long timeout) { String threadId = Thread.currentThread().getId().toString(); Boolean result = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeout, TimeUnit.SECONDS); // result 是 Boolean 类型，直接返回存在自动拆箱，为防止空指针不直接返回 return Boolean.TRUE.equals(result); } @Override public void unlock() { stringRedisTemplate.delete(KEY_PREFIX + name); }} 1234567891011121314151617181920/** * VERSION3.0 - 秒杀下单优惠券（通过分布式锁解决一人一单问题） */@Overridepublic CommonResult&lt;Long&gt; seckillVoucher(Long voucherId) { // 判断秒杀是否开始或结束、库存是否充足。 ... // 下单 SimpleDistributedLock4Redis lock = new SimpleDistributedLock4Redis(&quot;order:&quot; + UserHolder.getUser().getId(), stringRedisTemplate); boolean tryLock = lock.tryLock(TTL_TWO); ThrowUtils.throwIf(!tryLock, ErrorCode.OPERATION_ERROR, &quot;禁止重复下单&quot;); try { VoucherOrderService voucherOrderService = (VoucherOrderService) AopContext.currentProxy(); return voucherOrderService.createVoucherOrder(voucherId); } finally { lock.unlock(); }} 误删问题 123456789101112# 线程 1 获取到锁后执行业务，碰到了业务阻塞。setnx lock:order:1 thread01# 业务阻塞的时间超过了该锁的 TTL 时间，触发锁的超时释放。超时释放后，线程 2 获取到锁并执行业务。setnx lock:order:1 thread02# 线程 2 执行业务的过程中，线程 1 的业务执行完毕并且释放锁，但是释放的是线程 2 获取到的锁。（线程 2：你 TM 放我锁是吧！）del lock:order:1# 线程 3 获取到锁（此时线程 2 和 3 并行执行业务）setnx lock:order:1 thread03 解决方案：在线程释放锁时，判断当前这把锁是否属于自己，如果不属于自己，就不会进行锁的释放（删除）。 123456789101112# 线程 1 获取到锁后执行业务，碰到了业务阻塞。setnx lock:order:1 thread01# 业务阻塞的时间超过了该锁的 TTL 时间，触发锁的超时释放。超时释放后，线程 2 获取到锁并执行业务。setnx lock:order:1 thread02# 线程 2 执行业务的过程中，线程 1 的业务执行完毕并且释放锁。但是线程 1 需要判断这把锁是否属于自己，不属于自己就不会释放锁。# 于是线程 2 一直持有这把锁直到业务执行结束后才会释放，并且在释放时也需要判断当前要释放的锁是否属于自己。del lock:order:1# 线程 3 获取到锁并执行业务setnx lock:order:1 thread03 基于 Redis 的分布式锁的实现（解决误删问题） 相较于最开始分布式锁的实现，只需要增加一个功能：释放锁时需要判断当前锁是否属于自己。（而集群环境下不同 JVM 中的线程 ID 可能相同，增加一个 UUID 区分不同 JVM） 因此通过分布式锁存入 Redis 中的线程标识包括：UUID (服务器id)+ 线程 ID(线程id)。UUID 用于区分不同服务器中线程 ID 相同的线程，线程 ID 用于区分相同服务器的不同线程。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class SimpleDistributedLockBasedOnRedis implements DistributedLock { private String name; private StringRedisTemplate stringRedisTemplate; public SimpleDistributedLockBasedOnRedis(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } private static final String KEY_PREFIX = &quot;lock:&quot;; // ID_PREFIX 在当前 JVM 中是不变的，主要用于区分不同 JVM private static final String ID_PREFIX = UUID.randomUUID().toString(true) + &quot;-&quot;; /** * 获取锁 */ @Override public boolean tryLock(long timeoutSeconds) { // UUID 用于区分不同服务器中线程 ID 相同的线程；线程 ID 用于区分同一个服务器中的线程。 String threadIdentifier = ID_PREFIX + Thread.currentThread().getId(); Boolean isSucceeded = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadIdentifier, timeoutSeconds, TimeUnit.SECONDS); return Boolean.TRUE.equals(isSucceeded); } /** * 释放锁（释放锁前通过判断 Redis 中的线程标识与当前线程的线程标识是否一致，解决误删问题） */ @Override public void unlock() { // UUID 用于区分不同服务器中线程 ID 相同的线程；线程 ID 用于区分同一个服务器中的线程。 String threadIdentifier = THREAD_PREFIX + Thread.currentThread().getId(); String threadIdentifierFromRedis = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); // 比较 Redis 中的线程标识与当前的线程标识是否一致 if (!StrUtil.equals(threadIdentifier, threadIdentifierFromRedis)) { throw new BusinessException(ErrorCode.OPERATION_ERROR, &quot;释放锁失败&quot;); } // 释放锁标识 stringRedisTemplate.delete(KEY_PREFIX + name); }} 用Lua脚本解决原子性问题 分布式锁的原子性问题 线程 1 获取到锁并执行完业务，判断锁标识一致后释放锁，释放锁的过程中阻塞，导致锁没有释放成功，并且阻塞的时间超过了锁的 TTL 释放，导致锁自动释放。 此时线程 2 获取到锁，执行业务；在线程 2 执行业务的过程中，线程 1 完成释放锁操作。 之后，线程 3 获取到锁，执行业务，又一次导致此时有两个线程同时在并行执行业务。 因此，需要保证 unlock() 方法的原子性，即判断线程标识的一致性和释放锁这两个操作的原子性。 Redis 提供了 Lua 脚本功能，在一个脚本中编写多条 Redis 命令，确保 Redis 多条命令执行时的原子性。 unlock操作 12345678910111213141516private static final DefaultRedisScript&lt;Long&gt; UNLOCK_SCRIPT;static{//写成静态代码块，类加载就可以完成初始定义，就不用每次释放锁都去加载这个，性能提高咯 UNLOCK_SCRIPT = new DefaultRedisScript&lt;&gt;(); UNLOCK_SCRIPT.setLocation(new ClassPathResource(&quot;unlock.lua&quot;));//设置脚本位置 UNLOCK_SCRIPT.setResultType(Long.class);} public void unlock(){ //调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), ID_PREFIX + Thread.currentThread().getId() ); } Lua脚本 12345678-- 锁的key-- local key = KEYS[1]-- 当前线程标识-- local threadId = ARGV[1]-- 获取锁中的线程标识if(redis.call('get',KEYS[1]) == ARGV[1]) then return redis.call('del',KEYS[1])end return 0","link":"/posts/redis%E8%A7%A3%E5%86%B3%E5%B8%B8%E8%A7%81%E7%9A%84%E7%A7%92%E6%9D%80%E9%97%AE%E9%A2%98/"},{"title":"使用lua优化一人一单问题","text":"集群环境下的并发问题 分布式锁-原理 不去使用jvm内部的锁监视器，我们要在外部开一个锁监视器，让它监视所有的线程 Lua解决！ 解决这个问题主要是让一个用户不能同时创建两个订单(在缓存创建前) lua可以在保证原子性redis执行的原子性，所以咱可以用他解决一人一单引发的并发问题无需加锁 lua代码逻辑 12345678910111213141516171819202122232425262728293031-- 参数列表-- 优惠卷idlocal voucherId = ARGV[1]-- 用户idlocal userId = ARGV[2]-- 数据key-- 库存keylocal stockKey = 'seckill:stock:' .. voucherId-- 订单keylocal orderKey = 'seckill:order:' .. voucherIdprint(stockKey)print(orderKey)-- 脚本业务-- 检查库存是否充足if(tonumber(redis.call('get', stockKey)) &lt;= 0) then return 1end-- 检查用户是否已下单if(redis.call('sismember', orderKey, userId) == 1) then return 2end-- 扣库存redis.call('incrby', stockKey, -1)-- 下单保存用户redis.call('sadd', orderKey, userId)return 0 java调用代码逻辑 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private static final DefaultRedisScript&lt;Long&gt; SECKILL_SCRIPT; static { SECKILL_SCRIPT = new DefaultRedisScript&lt;&gt;(); SECKILL_SCRIPT.setLocation(new ClassPathResource(&quot;seckill.lua&quot;)); SECKILL_SCRIPT.setResultType(Long.class); } private IVoucherOrderService proxy; @Override @Transactional public Result seckillVoucher(Long voucherId) { //获取用户 Long userId = UserHolder.getUser().getId(); System.out.println(&quot;userId = &quot; + userId); System.out.println(&quot;voucherId = &quot; + voucherId); //执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString() ); //判断结果是否为零 assert result != null; int r = result.intValue(); if(r!=0){ return switch (r) { case 1 -&gt; Result.fail(&quot;库存不足&quot;); case 2 -&gt; Result.fail(&quot;不能重复下单&quot;); default -&gt; Result.fail(&quot;系统错误&quot;); }; } long orderId = redisIdWorker.nextId(&quot;order&quot;); // 保存阻塞队列 VoucherOrder voucherOrder = new VoucherOrder(); //订单id voucherOrder.setId(orderId); //用户id voucherOrder.setUserId(userId); //代金券id voucherOrder.setVoucherId(voucherId); //加入阻塞队列 orderTasks.add(voucherOrder); proxy = (IVoucherOrderService) AopContext.currentProxy(); //不为0 //为0 有购买资格 把下单信息保存到阻塞队列 //返回订单id return Result.ok(orderId); }","link":"/posts/%E4%BD%BF%E7%94%A8lua%E4%BC%98%E5%8C%96%E4%B8%80%E4%BA%BA%E4%B8%80%E5%8D%95%E9%97%AE%E9%A2%98/"},{"title":"redis读写一致问题","text":"Redis读写一致问题 条件: 数据库此时的数据为10,redis此时的数据也为10 业务流程: 操作数据库使得数据库的数据为20，删除redis里面的数据保证读写一致 先删缓存，再操作数据库 出现读写不一致情况: 线程1(业务) 线程2(并发线程) 删除缓存 查询缓存，没有命中，查询数据库(数据库查到为10，下一步将10写入redis) 将10写入缓存 更新数据库，将数据库中的数据改为20 最终情况 redis里面的数据 数据库里面的数据 10 20 出现数据不一致情况 先操作数据库，再删除缓存 线程1(并发线程) 线程2(业务线程) 查询缓存未命中，查询数据库(下一步:将缓存更新为10) 更新数据库 v=20 删除缓存 写入缓存数据10 最终情况: redis数据 数据库数据 10 20 两个方法选择原则 适用策略 典型场景 是否推荐使用延迟双删 先删缓存 → 后更新数据库 高一致性业务（余额、库存） ✅ 一定要延迟双删！ 先更新数据库 → 后删缓存 低一致性业务（资料、文章内容） ❌ 可以不用延迟双删 解决方案:双写一致性 读操作没啥问题按照老流程 延时双删 问题 答案 先删缓存还是先改数据库？ 先删缓存！ 避免并发写入旧值 为什么删两次？ 防止“改库之后，又有人写了旧值到缓存” 为什么要延迟删？ 给并发线程一个“写入脏缓存”的机会，然后再清理掉它 缺点: 问题点 延迟双删解决得了么？ 推荐改进方式 并发窗口写入脏缓存 ❌ 只能删最后一个 分布式锁 + 双删 / MQ 延迟时间难控制 ❌ 不可预测 MQ 或 Canal 机制更精准 异步删除失败风险 ❌ 会丢失删除 使用可靠任务队列 / Redis 持久化 操作复杂、代码维护困难 ❌ 容易遗漏 key 封装中间件、使用 AOP统一处理 给他加锁 读写都加锁 如图，程序运行串行化，性能低 引入共享锁和排他锁机制 共享锁：读锁readLock，加锁之后，其他线程可以共享读操作 排他锁：独占锁writeLock也叫，加锁之后，阻塞其他线程读写操作 代码Demo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import org.redisson.api.RReadWriteLock;import org.redisson.api.RedissonClient;import org.redisson.api.RLock;import java.util.concurrent.TimeUnit;public class UserService { private final RedissonClient redissonClient; private final RedisService redisService; // 你封装的 Redis 工具类 private final UserRepository userRepository; // 你操作数据库的类 public UserService(RedissonClient redissonClient, RedisService redisService, UserRepository userRepository) { this.redissonClient = redissonClient; this.redisService = redisService; this.userRepository = userRepository; } // 读操作：加“读锁” public User getUserById(Long userId) { String key = &quot;user:&quot; + userId; String lockKey = &quot;lock:user:&quot; + userId; RReadWriteLock rwLock = redissonClient.getReadWriteLock(lockKey); RLock readLock = rwLock.readLock(); try { readLock.lock(5, TimeUnit.SECONDS); // 加读锁，防止同时写入 User user = redisService.get(key); // 先查缓存 if (user != null) { return user; } // 缓存未命中 → 查数据库并回写缓存 user = userRepository.findById(userId); if (user != null) { redisService.set(key, user, 10, TimeUnit.MINUTES); } return user; } finally { readLock.unlock(); // 释放读锁 } } // 写操作：加“写锁” public void updateUser(User user) { Long userId = user.getId(); String key = &quot;user:&quot; + userId; String lockKey = &quot;lock:user:&quot; + userId; RReadWriteLock rwLock = redissonClient.getReadWriteLock(lockKey); RLock writeLock = rwLock.writeLock(); try { writeLock.lock(10, TimeUnit.SECONDS); // 加写锁，防止并发读/写 redisService.del(key); // 删除缓存（第一次） userRepository.save(user); // 更新数据库 // 第二次删除可延迟做（避免并发写入旧值） Thread.sleep(500); // 模拟延迟 redisService.del(key); // 延迟删除（第二次） } catch (InterruptedException e) { e.printStackTrace(); } finally { writeLock.unlock(); // 释放写锁 } }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import org.redisson.api.RReadWriteLock;import org.redisson.api.RedissonClient;import org.redisson.api.RLock;import java.util.concurrent.TimeUnit;public class UserService { private final RedissonClient redissonClient; private final RedisService redisService; // 你封装的 Redis 工具类 private final UserRepository userRepository; // 你操作数据库的类 public UserService(RedissonClient redissonClient, RedisService redisService, UserRepository userRepository) { this.redissonClient = redissonClient; this.redisService = redisService; this.userRepository = userRepository; } // 读操作：加“读锁” public User getUserById(Long userId) { String key = &quot;user:&quot; + userId; String lockKey = &quot;lock:user:&quot; + userId; RReadWriteLock rwLock = redissonClient.getReadWriteLock(lockKey); RLock readLock = rwLock.readLock(); try { readLock.lock(5, TimeUnit.SECONDS); // 加读锁，防止同时写入 User user = redisService.get(key); // 先查缓存 if (user != null) { return user; } // 缓存未命中 → 查数据库并回写缓存 user = userRepository.findById(userId); if (user != null) { redisService.set(key, user, 10, TimeUnit.MINUTES); } return user; } finally { readLock.unlock(); // 释放读锁 } } // 写操作：加“写锁” public void updateUser(User user) { Long userId = user.getId(); String key = &quot;user:&quot; + userId; String lockKey = &quot;lock:user:&quot; + userId; RReadWriteLock rwLock = redissonClient.getReadWriteLock(lockKey); RLock writeLock = rwLock.writeLock(); try { writeLock.lock(10, TimeUnit.SECONDS); // 加写锁，防止并发读/写 redisService.del(key); // 删除缓存（第一次） userRepository.save(user); // 更新数据库 // 第二次删除可延迟做（避免并发写入旧值） Thread.sleep(500); // 模拟延迟 redisService.del(key); // 延迟删除（第二次） } catch (InterruptedException e) { e.printStackTrace(); } finally { writeLock.unlock(); // 释放写锁 } }} 中间件解决方案 异步通知保证数据的最终一致性 canal是基于mysql的主从同步来实现的 二进制日志（BINLOG）记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但不包括数据查询（SELECT、SHOW）语句。","link":"/posts/redis%E8%AF%BB%E5%86%99%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/"},{"title":"redis持久化和数据淘汰方案","text":"Redis持久化方案 RDB RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据 Redis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下： 1234# 900秒内，如果至少有1个key被修改，则执行bgsave save 900 1 save 300 10 save 60 10000 RDB的执行原理 bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。fork采用的是copy-on-write技术： 当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作。 优点 解释 ✨ 内存效率高 只在写操作时拷贝数据，不会整块复制，大大节省内存 ✨ 服务不中断 主进程正常服务，RDB 快照异步生成 ✨ 快照性能好 子进程写磁盘，避免与主线程争资源 ✨ 快照数据一致 基于 fork 时刻的数据，具有一致性视图 如果直接用save命令会阻塞Redis主线程，性能不太好 AOF AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 AOF文件会记录指令 AOF的配置 AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF： 1234# 是否开启AOF功能，默认是noappendonly yes# AOF文件的名称appendfilename &quot;appendonly.aof&quot; AOF的命令记录的频率也可以通过redis.conf文件来配： 123456# 表示每执行一次写命令，立即记录到AOF文件appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘appendfsync no 配置项 刷盘时机 优点 缺点 Always 同步刷盘 可靠性高，几乎不丢数据 性能影响大 everysec 每秒刷盘 性能适中 最多丢失1秒数据 no 操作系统控制 性能最好 可靠性较差，可能丢失大量数据 因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。(并且文件会进行压缩乱码) Redis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置： 1234# AOF文件比上次文件 增长超过多少百分比则触发重写auto-aof-rewrite-percentage 100# AOF文件体积最小多大以上才触发重写auto-aof-rewrite-min-size 64mb 两种方式的比较 RDB AOF 持久化方式 定时对整个内存做快照 记录每一次执行的命令 数据完整性 不完整，两次备份之间会丢失 相对完整，取决于刷盘策略 文件大小 会有压缩，文件体积小 记录命令，文件体积很大 宕机恢复速度 很快 慢 数据恢复优先级 低，因为数据完整性不如AOF 高，因为数据完整性更高 系统资源占用 高，大量CPU和内存消耗 低，主要是磁盘IO资源但AOF重写时会占用大量CPU和内存资源 使用场景 可以容忍数分钟的数据丢失，追求更快的启动速度 对数据安全性要求较高常见 数据淘汰方案 惰性删除 惰性删除：设置该key过期时间后，我们不去管它，当需要该key时，我们在检查其是否过期，如果过期，我们就删掉它，反之返回该key 12set name zhangsan get name //发现name过期了，直接删除key 优点 ：对CPU友好，只会在使用该key时才会进行过期检查，对于很多用不到的key不用浪费时间进行过期检查 缺点 ：对内存不友好，如果一个key已经过期，但是一直没有使用，那么该key就会一直存在内存中，内存永远不会释放 定期删除 定期删除：每隔一段时间，我们就对一些key进行检查，删除里面过期的key(从一定数量的数据库中取出一定数量的随机key进行检查，并删除其中的过期key)。 定期清理有两种模式： SLOW模式是定时任务，执行频率默认为10hz，每次不超过25ms，以通过修改配置文件redis.conf 的hz 选项来调整这个次数 FAST模式执行频率不固定，但两次间隔不低于2ms，每次耗时不超过1ms 优点：可以通过限制删除操作执行的时长和频率来减少删除操作对 CPU 的影响。另外定期删除，也能有效释放过期键占用的内存。 缺点：难以确定删除操作执行的时长和频率。 Redis的过期删除策略：惰性删除 + 定期删除两种策略进行配合使用","link":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/"},{"title":"机器学习笔记","text":"特征预处理 特征预处理分为标准化和归一化 为什么进行归一化、标准化？ 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响（支配）目标结果，使得一些模型（算法）无法学习到其它的特征。 归一化 通过对原始数据进行变换把数据映射到【mi,mx】(默认为[0,1])之间 $$ \\begin{cases} X’=\\frac{x-x_{min}}{x_{max}-x_{min}}\\ X’’=X’*(m_x-m_i)+mi \\end{cases} $$ 理解: X’是为了拿到x_min到x_max的长度占比 X’'是为了拿到新的区间内长度+上左区间=新的x值 api: 数据归一化的API实现 1sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)… ) feature_range 缩放区间 调用 fit_transform(X) 将特征进行归一化缩放 标准化 通过对原始数据进行标准化，转换为均值为0标准差为1的标准正态分布的数据 $$ \\frac{x-mean}{\\sigma} $$ mean 为特征的平均值 σ 为特征的标准差 数据标准化的API实现 1sklearn.preprocessing. StandardScaler() 调用 fit_transform(X) 将特征进行归一化缩放 超参数选择 交叉验证 交叉验证是一种数据集的分割方法，将训练集划分为 n 份，其中一份做验证集、其他n-1份做训练集集 交叉验证法原理：将数据集划分为 cv=10 份： 1.第一次：把第一份数据做验证集，其他数据做训练 2.第二次：把第二份数据做验证集，其他数据做训练 3… 以此类推，总共训练10次，评估10次。 4.使用训练集+验证集多次评估模型，取平均值做交叉验证为模型得分 5.若k=5模型得分最好，再使用全部训练集(训练集+验证集) 对k=5模型再训练一边，再使用测试集对k=5模型做评估 网格搜索法 假设我们有两个超参数： n_neighbors = [3, 5, 7, 9] weights = [“uniform”, “distance”] 我们就是在穷举所有组合，可以画成这样一个小表格： n_neighbors weights 组合编号 3 uniform 1 3 distance 2 5 uniform 3 5 distance 4 7 uniform 5 7 distance 6 9 uniform 7 9 distance 8 可以看出我们的网格搜索法就是将所有的存在的超参数组合进行训练 优点： 简单直观，易于理解和实现。 能够系统地探索超参数空间。 缺点： 当超参数空间较大时，计算成本非常高。 可能无法找到全局最优解，尤其是在超参数空间不连续或存在多个局部最优解时。 交叉验证网格搜索的API: 运用案例 1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn.datasets import load_iris # 加载鸢尾花测试集的.from sklearn.model_selection import train_test_split, GridSearchCV # 分割训练集和测试集的, 网格搜索的from sklearn.preprocessing import StandardScaler # 数据标准化的from sklearn.neighbors import KNeighborsClassifier # KNN算法 分类对象from sklearn.metrics import accuracy_score # 模型评估的, 计算模型预测的准确率# 1. 获取数据集.iris_data = load_iris()# 2. 数据基本处理-划分数据集.x_train, x_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=22)# 3. 数据集预处理-数据标准化.transfer = StandardScaler()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4. 模型训练.# 4.1 创建估计器对象.estimator = KNeighborsClassifier()# 4.2 使用校验验证网格搜索. 指定参数范围.param_grid = {&quot;n_neighbors&quot;: range(1, 10)}# 4.3 具体的 网格搜索过程 + 交叉验证.# 参1: 估计器对象, 参2: 参数范围, 参3: 交叉验证的折数.estimator = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5)# 具体的模型训练过程.estimator.fit(x_train, y_train)# 4.4 交叉验证, 网格搜索结果查看.print(estimator.best_score_) # 模型在交叉验证中, 所有参数组合中的最高平均测试得分print(estimator.best_estimator_) # 最优的估计器对象.print(estimator.cv_results_) # 模型在交叉验证中的结果.print(estimator.best_params_) # 模型在交叉验证中的结果.# 5. 得到最优模型后, 对模型重新预测.estimator = KNeighborsClassifier(n_neighbors=6)estimator.fit(x_train, y_train)print(f'模型评估: {estimator.score(x_test, y_test)}') # 因为数据量和特征的问题, 该值可能小于上述的平均测试得分. KNN KNN算法定义： K-近邻算法（K Nearest Neighbor，简称KNN） KNN算法思想：如果一个样本在特征空间中的 k 个最相似的样本中的大多数属于某一个类别，则该样本也属于这个类别 (其实就是看你周围样本的特征参数进行选举或加权平均来判断你大概为什么标签) 样本相似性：样本都是属于一个任务数据集的。样本距离越近则越相似。 API的介绍 KNN分类API： 1sklearn.neighbors.KNeighborsClassifier(n_neighbors=5) n_neighbors：int,可选（默认= 5），k_neighbors查询默认使用的邻居数 KNN回归API: 1sklearn.neighbors.KNeighborsRegressor(n_neighbors=5) 距离度量方法 欧式距离 $$ \\sqrt(x_i^2-x_y^2) $$ 曼哈顿距离 为啥叫曼哈顿距离呢？其实他有个别名叫城市距离，纽约曼哈顿区城市街区比较方方正正。 $$ |x_i-y_i| $$ 切比雪夫距离 曼哈顿距离的变种 闵氏距离 对上面三种度量方法的一个结合 KNN案例之手写数字识别 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import matplotlib.pyplot as pltimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifierimport joblibfrom collections import Counter# 1. 显示图片.def show_digit(idx): # 1.1 加载数据. data = pd.read_csv('手写数字识别.csv') # 1.2非法值校验. if idx &lt; 0 or idx &gt; len(data) - 1: return # 1.3 打印数据基本信息 x = data.iloc[:, 1:] y = data.iloc[:, 0] print(f'数据基本信息: {x.shape})') print(f'类别数据比例: {Counter(y)}') # 显示图片 # 1.4 将数据形状修改为: 28*28 digit = x.iloc[idx].values.reshape(28, 28) # 1.5 关闭坐标轴标签 plt.axis('off') # 1.6 显示图像 plt.imshow(digit, cmap='gray') # 灰色显示 plt.show()# 2. 训练模型.def train_model(): # 1. 加载数据. data = pd.read_csv('手写数字识别.csv') x = data.iloc[:, 1:] y = data.iloc[:, 0] # 2.数据预处理, 归一化. x = x / 255 # 3. 分割训练集和测试集. # stratify: 按照y的类别比例进行分割 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=21) # 4. 训练模型 estimator = KNeighborsClassifier(n_neighbors=3) estimator.fit(x_train, y_train) # 5. 模型评估 my_score = estimator.score(x_test, y_test) print(f'测试集准确率为: {my_score:.2f}') # 6. 模型保存. joblib.dump(estimator, 'model/knn.pth')# 3. 测试模型.def use_model(): # 1. 读取图片 img = plt.imread('data/demo.png') # 灰度图, 28*28像素 plt.imshow(img, cmap='gray') plt.show() # 2. 加载模型. estimator = joblib.load('model/knn.pth') # 3. 预测图片. img = img.reshape(1, -1) # 形状从: (28, 28) =&gt; (1, 784) # print(img.shape) y_test = estimator.predict(img) print(f'您绘制的数字是: {y_test}')# 在main函数中测试if __name__ == '__main__': # 1. 调用函数, 查看图片. # show_digit(0) # show_digit(10) # show_digit(100) # 2. 训练模型. # train_model() # 3. 测试模型 use_model() 线性回归 线性回归(Linear regression)是利用 回归方程(函数) 对 一个或多个自变量(特征值)和因变量(目标值)之间 关系进行建模的一种分析方式。 损失函数最优解(MSE为例子) 我们可以拟合无数条线性函数，所以我们需要一个标准来判断哪个线性函数是最好的 误差概念：用预测值y – 真实值y就是误差 损失函数：衡量每个样本预测值与真实值效果的函数 红色直线能更好的拟合所有点”也就是误差最小，误差和最小 一元线性方程解析解 多元方程组解 梯度下降法 定义 • 求解函数极值还有更通用的方法就是梯度下降法。顾名思义:沿着梯度下降的方向求解极小值 • 举个例子:坡度最陡下山法 输入:初始化位置S;每步距离为a 。输出:从位置S到达山底 步骤1:令初始化位置为山的任意位置S 步骤2:在当前位置环顾四周，如果四周都比S高返回S;否则执行步骤3 步骤3: 在当前位置环顾四周，寻找坡度最陡的方向，令其为x方向 步骤4:沿着x方向往下走，长度为a，到达新的位置S‘ 步骤5:在S‘位置环顾四周，如果四周都比S‘高，则返回S‘。否则转到步骤3 小结:通过循环迭代的方法不断更新位置S (相当于不断更新权重参数w) 最终找到最优解 这个方法可用来求损失函数最优解， 比正规方程更通用 银行信贷案例 梯度下降算法分类 正规方程和梯度下降算法的对比 回归评估方法 平均绝对误差 Mean Absolute Error (MAE) $$ MAE=\\frac{1}{m}\\sum_{i=1}^m|y_i-\\hat{y}| $$ 上面的公式中：n 为样本数量, y 为实际值, y_hat 为预测值 MAE 越小模型预测约准确 Sklearn 中MAE的API 12from sklearn.metrics import mean_absolute_errormean_absolute_error(y_test,y_predict) 均方误差 Mean Squared Error (MSE) $$ MSE=\\frac{1}{2m}\\sum_{i=1}^m(y_i-\\hat{y})^2 $$ Sklearn 中MSE的API 12from sklearn.metrics import mean_squared_errormean_squared_error(y_test,y_predict) 均方根误差 Root Mean Squared Error (RMSE) $$ RMSE=\\sqrt{\\frac{1}{m}\\sum_{i=1}^m(y_i-\\hat{y})^2} $$ 指标 对大误差态度 是否敏感异常值 单位 常见场景 MAE 温柔 不敏感 和 y 一样 数据噪声大 MSE 很狠 极敏感 y² 训练优化 RMSE 很狠 极敏感 和 y 一样 评估模型 线性回归房价预测案例 123456789101112131415161718192021222324252627282930313233343536# 0.导包from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegression,SGDRegressorfrom sklearn.metrics import mean_squared_error# 1.加载数据boston = load_boston()# print(boston)# 2.数据集划分x_train,x_test,y_train,y_test =train_test_split(boston.data,boston.target,test_size=0.2,random_state=22)# 3.标准化process=StandardScaler()x_train=process.fit_transform(x_train)x_test=process.transform(x_test)# 4.模型训练# 4.1 实例化(正规方程)# model =LinearRegression(fit_intercept=True)model = SGDRegressor(learning_rate='constant',eta0=0.01)# 4.2 fitmodel.fit(x_train,y_train)# print(model.coef_)# print(model.intercept_)# 5.预测y_predict=model.predict(x_test)print(y_predict)# 6.模型评估print(mean_squared_error(y_test,y_predict)) 逻辑回归 主要是应用于二分类问题 数学模型 sigmoid函数 $$ f(x)=\\frac{1}{1+e^{wx+b}}\\ f(x)’=f(x)(1-f(x))\\ $$ 数学性质: 是单调递增函数，拐点为(0,0.5) 极大似然估计 核心思想： 设模型中含有待估参数w，可以取很多值。已经知道了样本观测值，从w的一切可能值中选出一个使该观察值出现的概率为最大的值，作为w参数的估计值，这就是极大似然估计。（顾名思义：就是看上去那个是最大可能的意思） 举个例子： 假设有一枚不均匀的硬币，出现正面的概率和反面的概率是不同的。假定出现正面的概率为𝜃， 抛了6次得到如下现象 D = {正面，反面，反面，正面，正面，正面}。每次投掷事件都是相互独立的。 则根据产生的现象D，来估计参数𝜃是多少? 123P(D|𝜃) = P {正面，反面，反面，正面，正面，正面} = P(正面|𝜃) P(反面|𝜃) P(反面|𝜃) P(正面|𝜃) P(正面|𝜃) P(正面|𝜃) =𝜃 *(1-𝜃)*(1-𝜃)𝜃*𝜃*𝜃 = 𝜃^4(1 − 𝜃)^2 问题转换为求极大值时𝜃是多少 逻辑回归原理 逻辑回归概念 Logistic Regression • 一种分类模型，把线性回归的输出，作为逻辑回归的输入。 • 输出是(0, 1)之间的值 • 基本思想 利用线性模型 f(x) = wx + b 根据特征的重要性计算出一个值 再使用 sigmoid 函数将 f(x) 的输出值映射为概率值 设置阈值(eg:0.5)，输出概率值大于 0.5，则将未知样本输出为 1 类 否则输出为 0 类 3.逻辑回归的假设函数 h(w) = sigmoid(wx + b ) 线性回归的输出，作为逻辑回归的输入 损失函数 $$ Loss(L)=-\\sum_{i=1}^m(y_ilog(p_i)+(1-y_i)log(1-p_i)) $$ 逻辑回归电信客户流失预测案例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_score, roc_auc_score# 1. 定义函数, 表示: 数据基本处理def dm01_数据基本处理(): # 1. 读取数据, 查看数据的基本信息. churn_pd = pd.read_csv('data/churn.csv') # churn_pd.info() # print(f'churn_pd.describe(): {churn_pd.describe()}') # print(f'churn_pd: {churn_pd}') # 2. 处理类别型的数据, 类别型数据做 one-hot编码(热编码). churn_pd = pd.get_dummies(churn_pd) churn_pd.info() # print(f'churn_pd: {churn_pd}') # 3. 去除列 Churn_No, gender_Male churn_pd.drop(['Churn_No', 'gender_Male'], axis=1, inplace=True) # 按列删除 print(f'churn_pd: {churn_pd}') # 4. 列标签重命名, 打印列名 churn_pd.rename(columns={'Churn_Yes': 'flag'}, inplace=True) print(f'列名: {churn_pd.columns}') # 5. 查看标签的分布情况 0.26用户流失 value_counts = churn_pd.flag.value_counts() print(value_counts)# 2. 定义函数, 表示: 特征筛选def dm02_特征筛选(): # 1. 读取数据 churn_pd = pd.read_csv('data/churn.csv') # 2. 处理类别型的数据, 类别型数据做 one-hot编码(热编码). churn_pd = pd.get_dummies(churn_pd) # 3. 去除列 Churn_No, gender_Male churn_pd.drop(['Churn_No', 'gender_Male'], axis=1, inplace=True) # 4. 列标签重命名 churn_pd.rename(columns={'Churn_Yes': 'flag'}, inplace=True) # 5. 查看标签的分布情况 value_counts = churn_pd.flag.value_counts() print(value_counts) # 6. 查看Contract_Month 是否预签约流失情况 sns.countplot(data=churn_pd, x='Contract_Month', hue='flag') plt.show()# 3. 定义函数, 表示: 模型训练 和 评测def dm03_模型训练和评测(): # 1. 读取数据 churn_pd = pd.read_csv('data/churn.csv') # 2. 数据预处理 # 2.1 处理类别型的数据, 类别型数据做 one-hot编码(热编码). churn_pd = pd.get_dummies(churn_pd) # 2.2 去除列 Churn_No, gender_Male churn_pd.drop(['Churn_No', 'gender_Male'], axis=1, inplace=True) # 2.3 列标签重命名 churn_pd.rename(columns={'Churn_Yes': 'flag'}, inplace=True) # 3. 特征处理. # 3.1 提取特征和标签 x = churn_pd[['Contract_Month', 'internet_other', 'PaymentElectronic']] y = churn_pd['flag'] # 3.2 训练集和测试集的分割 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=21) # 4. 模型训练. estimator = LogisticRegression() estimator.fit(x_train, y_train) # 5. 模型预测 y_predict = estimator.predict(x_test) print(f'预测结果: {y_predict}') # 6. 模型评估 print(f'准确率: {accuracy_score(y_test, y_predict)}') print(f'准确率: {estimator.score(x_test, y_test)}') # 计算AUC值. print(f'AUC值: {roc_auc_score(y_test, y_predict)}')if __name__ == '__main__': # dm01_数据基本处理() # dm02_特征筛选() dm03_模型训练和评测()","link":"/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"categories":[{"name":"Hexo教程","slug":"Hexo教程","link":"/categories/Hexo%E6%95%99%E7%A8%8B/"},{"name":"redis笔记","slug":"redis笔记","link":"/categories/redis%E7%AC%94%E8%AE%B0/"},{"name":"redis教程","slug":"redis教程","link":"/categories/redis%E6%95%99%E7%A8%8B/"},{"name":"redis的应用","slug":"redis的应用","link":"/categories/redis%E7%9A%84%E5%BA%94%E7%94%A8/"},{"name":"redis的问题方案","slug":"redis的问题方案","link":"/categories/redis%E7%9A%84%E9%97%AE%E9%A2%98%E6%96%B9%E6%A1%88/"},{"name":"mysql教程","slug":"mysql教程","link":"/categories/mysql%E6%95%99%E7%A8%8B/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"}]}